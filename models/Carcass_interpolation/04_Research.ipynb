{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative research\n",
    "\n",
    "In this advanced notebook, we apply the [carcass interpolation model](./01_Demo_E.ipynb), as well as [horizon extension](./../Horizon_extension/Demo_E.ipynb) and enhancement ones in a quick succesion with the help of [research](./../Research_template.ipynb). It is adviced to check out our notebooks on these techniques prior to looking at this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../..')\n",
    "from seismiqb.batchflow.research import Research, Option, FileLogger\n",
    "from seismiqb.batchflow.research import RC, KV\n",
    "\n",
    "from seismiqb import SeismicGeometry, Horizon, HorizonMetrics, plot_image\n",
    "\n",
    "from seismiqb.src.controllers.torch_models import EncoderDecoder, ExtensionModel\n",
    "from seismiqb import MODEL_CONFIG_DETECTION, MODEL_CONFIG_EXTENSION\n",
    "from seismiqb import BaseController, Interpolator, Enhancer, Extender\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "DEVICES = [0, 1, 2, 3, 4, 5, 6, 7]         # physical device numbers\n",
    "WORKERS = len(DEVICES)\n",
    "\n",
    "RESEARCH_NAME = 'Research_horizons'\n",
    "\n",
    "DUMP_NAME = date.today().strftime(\"%Y-%m-%d\") + RESEARCH_NAME[8:]\n",
    "N_REPS = 4\n",
    "\n",
    "FREQUENCIES = (200, 200)\n",
    "ITERATIONS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    'savedir': None,\n",
    "    'logger': None,\n",
    "    'monitor': False,\n",
    "    'bar': False,\n",
    "    'plot': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "DETECTION_CONFIG = {\n",
    "    'savedir': None,\n",
    "    'monitor': True,\n",
    "    'bar': False,\n",
    "    'plot': None,\n",
    "    'sampler': {},\n",
    "    'train': {\n",
    "        'model_class': EncoderDecoder,\n",
    "        'model_config': {\n",
    "            **MODEL_CONFIG_DETECTION\n",
    "        },\n",
    "        \n",
    "        'batch_size': 64,\n",
    "        'crop_shape': (1, 256, 256),\n",
    "\n",
    "        'adaptive_slices': True,\n",
    "        'grid_src': 'quality_grid',\n",
    "        'side_view': False,\n",
    "        'width': 3,\n",
    "\n",
    "        'rebatch_threshold': 0.8,\n",
    "        'rescale_batch_size': True,\n",
    "        \n",
    "        'prefetch': 2,\n",
    "        'n_iters': 500,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "    'inference': {\n",
    "        'orientation': 'ix',\n",
    "        'batch_size': 64,\n",
    "        'crop_shape': (1, 256, 256),\n",
    "        'overlap_factor': 2.0,\n",
    "        'prefetch': 0,\n",
    "        \n",
    "        'chunk_size': 100,\n",
    "        'chunk_overlap': 0.05,\n",
    "    },\n",
    "    'evaluate': {\n",
    "        'n': 1,\n",
    "        'supports': 100,\n",
    "        'device': 'cpu',\n",
    "        'dump': True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_CONFIG = {\n",
    "    'savedir': None,\n",
    "    'monitor': True,\n",
    "    'bar': False,\n",
    "    'plot': False,\n",
    "    'sampler': {},\n",
    "    'train': {\n",
    "        'model_class': EncoderDecoder,\n",
    "        'model_config': {\n",
    "            **MODEL_CONFIG_EXTENSION,\n",
    "            'microbatch': 16,\n",
    "        },\n",
    "        \n",
    "        'batch_size': 64,\n",
    "        'crop_shape': (1, 96, 96),\n",
    "\n",
    "        'adaptive_slices': False,\n",
    "        'side_view': True,\n",
    "        'width': 3,\n",
    "\n",
    "        'rebatch_threshold': 0.7,\n",
    "        'rescale_batch_size': True,\n",
    "        \n",
    "        'prefetch': 3,\n",
    "        'n_iters': 300,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "    'inference': {\n",
    "        'batch_size': 128,\n",
    "        'crop_shape': (1, 96, 96),\n",
    "        'prefetch': 0,\n",
    "        \n",
    "        'width': 3,\n",
    "        \n",
    "        'n_steps': 50,\n",
    "        'stride': 16,\n",
    "    },\n",
    "    'evaluate': {\n",
    "        'n': 1,\n",
    "        'supports': 100,\n",
    "        'dump': True,\n",
    "        'device': 'cpu',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    ('/data/seismic_data/seismic_interpretation/CUBE_01_ETP/amplitudes_01_ETP.qblosc',\n",
    "     '/data/seismic_data/seismic_interpretation/CUBE_01_ETP/INPUTS/HORIZONS/RAW/etp*'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled = [\n",
    "    (cube_path, horizon_path)\n",
    "    for cube_path, horizon_dir in paths\n",
    "    for horizon_path in glob(horizon_dir)\n",
    "    if not horizon_path.endswith('.dvc')\n",
    "]\n",
    "\n",
    "options = [\n",
    "    KV((cube_path, horizon_path),\n",
    "       '+'.join((cube_path.split('/')[-1].split('.')[0],\n",
    "                 horizon_path.split('/')[-1].split('.')[0])))\n",
    "    for cube_path, horizon_path in unrolled\n",
    "]\n",
    "random.shuffle(options)\n",
    "\n",
    "domain = Option('cube_and_horizon', options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_experiment(config):\n",
    "    start_time = perf_counter()\n",
    "    \n",
    "    ###################################################################################\n",
    "    ################################   PARSE CONFIGS   ################################\n",
    "    ###################################################################################\n",
    "    # Get all the params from configs\n",
    "    config = config.config()\n",
    "    cube_path, horizon_path = config['cube_and_horizon']\n",
    "    n_rep = config['repetition']\n",
    "    \n",
    "    # Directory to save results to\n",
    "    results_dir = os.path.join(RESEARCH_NAME, 'custom_results')\n",
    "    \n",
    "    short_name_cube = cube_path.split('/')[-1].split('.')[0]\n",
    "    short_name_horizon = horizon_path.split('/')[-1].split('.')[0]\n",
    "    alias = os.path.join(short_name_cube, short_name_horizon, f'{n_rep}')\n",
    "    savedir = os.path.join(results_dir, alias)\n",
    "    \n",
    "    return_value = [[], [], [], []]   # coverages, window ratios, support corrs, phases\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "    ####################################    BASE    ###################################\n",
    "    ###################################################################################\n",
    "    base_config = {\n",
    "        **BASE_CONFIG,\n",
    "        'savedir': savedir,\n",
    "    }\n",
    "    controller = BaseController(base_config)\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "    ##################################   DETECTION   ##################################\n",
    "    ###################################################################################\n",
    "    detection_config = {\n",
    "        **DETECTION_CONFIG,\n",
    "        'savedir': f'{savedir}/0_detection',\n",
    "        'logger': controller.filelogger,\n",
    "    }\n",
    "    detector = Interpolator(detection_config)\n",
    "    \n",
    "    train_dataset = detector.make_dataset(cube_paths=cube_path, horizon_paths=horizon_path)\n",
    "    model = detector.train(dataset=train_dataset, frequencies=FREQUENCIES)\n",
    "    prediction = detector.inference(train_dataset, model)\n",
    "    horizon = detector.postprocess(prediction)\n",
    "    \n",
    "    info = detector.evaluate(prediction, dataset=train_dataset)[0]\n",
    "    \n",
    "    return_value[0].append(horizon.coverage)\n",
    "    return_value[1].append(info['window_rate'])\n",
    "    return_value[2].append(info['corrs'])\n",
    "    return_value[3].append(info['phase'])\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "    ##################################   EXTENSION   ##################################\n",
    "    ###################################################################################\n",
    "    for i in range(ITERATIONS):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        extension_config = {\n",
    "            **EXTENSION_CONFIG,\n",
    "            'savedir': f'{savedir}/{1+i}_extension',\n",
    "            'logger': controller.filelogger,\n",
    "        }\n",
    "        extender = Extender(extension_config)\n",
    "        \n",
    "        model = extender.train(horizon=horizon)\n",
    "        horizon = extender.inference(horizon, model)\n",
    "        horizon = extender.postprocess(horizon)\n",
    "        \n",
    "        info = extender.evaluate(horizon, dataset=train_dataset)[0]\n",
    "        \n",
    "        return_value[0].append(horizon.coverage)\n",
    "        return_value[1].append(info['window_rate'])\n",
    "        return_value[2].append(info['corrs'])\n",
    "        return_value[3].append(info['phase'])\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    ##############################   SAVE NEXT TO CUBE   ##############################\n",
    "    ###################################################################################\n",
    "    cube_dir = os.path.dirname(horizon.geometry.path)\n",
    "    savepath = os.path.join(cube_dir, 'PREDICTIONS/HORIZONS', DUMP_NAME)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    horizon.name = '+' + horizon.name.replace('enhanced_', '').replace('extended_', '')\n",
    "    if N_REPS != 1:\n",
    "        horizon.name += f'_{n_rep}'\n",
    "\n",
    "    savepath = os.path.join(savepath, horizon.name)\n",
    "    horizon.dump_float(savepath, add_height=False)\n",
    "    detector.log(f'Dumped horizon to {savepath}')\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    ###################################   RETURNS   ###################################\n",
    "    ###################################################################################\n",
    "    return_value.append(perf_counter() - start_time)\n",
    "    \n",
    "    msg = f'Finished experiment:\\n{\"\"*60}{horizon.name}\\n'\n",
    "    for name, value in zip(returned_values, return_value):\n",
    "        msg += f'{\"\"*60}{name} -> {value}\\n'\n",
    "    detector.log(msg)\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {RESEARCH_NAME}\n",
    "\n",
    "returned_values = [\n",
    "    'coverages', 'window_rates',\n",
    "    'corrs', 'phases',\n",
    "    'total_time'\n",
    "]\n",
    "\n",
    "research = (\n",
    "    Research()\n",
    "    .add_logger(FileLogger)\n",
    "    .init_domain(domain, n_reps=N_REPS)\n",
    "    .add_callable(perform_one_experiment,           # Callable to run\n",
    "                  returns=returned_values,          # Names of returned results\n",
    "                  execute='#0',                     # Execute immediately\n",
    "                  config=RC(),                      # Pass config to the callable\n",
    "                  name='perform_one_experiment')    # Name to be shown in the dataframe\n",
    ")\n",
    "\n",
    "research.run(\n",
    "    n_iters=1, timeout=10000,\n",
    "    name=RESEARCH_NAME, bar=True,\n",
    "    workers=WORKERS, devices=DEVICES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average horizons\n",
    "If each carcass is interpolated multiple times, we can aggregate repetitions into an averaged surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_REPS > 1:\n",
    "\n",
    "    for cube_path, _ in paths:\n",
    "        # Parse paths and make directory for saving averaged horizons\n",
    "        cube_dir = os.path.dirname(cube_path)\n",
    "        horizon_dir = os.path.join(cube_dir, 'PREDICTIONS/HORIZONS', DUMP_NAME)\n",
    "        savedir = horizon_dir + '_AVERAGED'\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        # Load all the predictions for a given cube\n",
    "        geometry = SeismicGeometry(cube_path)\n",
    "        horizons = [Horizon(path, geometry) for path in glob(horizon_dir + '/*')]\n",
    "        names = set(['_'.join(horizon.name.split('_')[:-1]) for horizon in horizons])\n",
    "\n",
    "        # Average\n",
    "        for name in sorted(names):\n",
    "            current_horizons = [horizon for horizon in horizons\n",
    "                                if horizon.name.startswith(name)]\n",
    "\n",
    "            averaged, dct = Horizon.average_horizons(current_horizons)\n",
    "            plot_image(dct['std_matrix'], title=f'Averaged {name}')\n",
    "            plt.show()\n",
    "\n",
    "            savepath = os.path.join(savedir, name)\n",
    "            averaged.dump_float(savepath)\n",
    "            print('Dumped to', savepath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the best horizon\n",
    "If there are multiple repetitions, we can select the best one, based on metrics (support correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_REPS > 1:\n",
    "\n",
    "    for cube_path, _ in paths:\n",
    "        # Parse paths and make directory for saving averaged horizons\n",
    "        cube_dir = os.path.dirname(cube_path)\n",
    "        horizon_dir = os.path.join(cube_dir, 'PREDICTIONS/HORIZONS', DUMP_NAME)\n",
    "        averaged_dir = horizon_dir + '_AVERAGED'\n",
    "        \n",
    "        savedir = horizon_dir + '_BEST'\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        # Load all the predictions for a given cube, as well as averaged versions of them\n",
    "        geometry = SeismicGeometry(cube_path)\n",
    "        horizons = [Horizon(path, geometry) for path in glob(horizon_dir + '/*')]\n",
    "        averaged = [Horizon(path, geometry) for path in glob(averaged_dir + '/*')]\n",
    "        names = set(['_'.join(horizon.name.split('_')[:-1]) for horizon in horizons])\n",
    "\n",
    "        # Select the best one\n",
    "        for name in sorted(names):\n",
    "            current_horizons = [horizon for horizon in horizons\n",
    "                                if horizon.name.startswith(name)]\n",
    "            current_horizons += [horizon for horizon in averaged\n",
    "                                 if name in horizon.name]\n",
    "            \n",
    "            values = []\n",
    "            for horizon in current_horizons:\n",
    "                hm = HorizonMetrics(horizon)\n",
    "                correlation_map = hm.evaluate('support_corrs', supports=100,\n",
    "                                              device='gpu', plot=False, show=False)\n",
    "                values.append(np.nanmean(correlation_map))\n",
    "            idx = np.argmax(values)\n",
    "            best = current_horizons[idx]\n",
    "\n",
    "            savepath = os.path.join(savedir, name)\n",
    "            best.dump_float(savepath)\n",
    "            print(f'Dumped {idx} to {savepath}')\n",
    "            print(f'MeanMetrics are {[round(item, 3) for item in values]}')\n",
    "            print(f'Coverages   are {[round(horizon.coverage, 3) for horizon in current_horizons]}\\n')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
